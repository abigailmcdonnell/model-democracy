{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0k0zd30ew-a",
        "outputId": "f1b0c1a5-7cb5-495e-fd80-3c12ff0b1eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting importlib-metadata==4.13.0\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata==4.13.0) (3.18.1)\n",
            "Installing collected packages: importlib-metadata\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "Successfully installed importlib-metadata-4.13.0\n",
            "Collecting xarray==0.20.1\n",
            "  Downloading xarray-0.20.1-py3-none-any.whl (835 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m835.7/835.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from xarray==0.20.1) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.10/dist-packages (from xarray==0.20.1) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->xarray==0.20.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->xarray==0.20.1) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->xarray==0.20.1) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1->xarray==0.20.1) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install importlib-metadata==4.13.0\n",
        "! pip install xarray==0.20.1\n",
        "! pip install --upgrade xarray cftime nc-time-axis\n",
        "! pip install netCDF4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nO-UKkIe3rl"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "import sys\n",
        "import netCDF4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLWZJNe2d6Qj"
      },
      "source": [
        "Plotting design specifications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvwfJ_X1HVAB"
      },
      "outputs": [],
      "source": [
        "# plotting parameters\n",
        "color_list = ['#000000', '#648FFF', '#785EF0', '#DC267F', '#FE6100', '#FFB000', '#D55E00', '#CC79A7'] * 2\n",
        "marker_list = ['o', 's', 'P', '+', 'D', 'v', '3', 'm'] * 2\n",
        "markersize = 6\n",
        "linewidth = 2\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "cdds_params={'axes.linewidth': 3,\n",
        " 'axes.axisbelow': False,\n",
        " 'axes.edgecolor': 'black',\n",
        " 'axes.facecolor': 'None',\n",
        " 'axes.grid': False,\n",
        " 'axes.labelcolor': 'black',\n",
        " 'axes.spines.right': False,\n",
        " 'axes.spines.top': False,\n",
        " 'axes.titlesize': 20,\n",
        " 'axes.labelsize': 20,\n",
        " 'axes.titlelocation': 'left',\n",
        " 'figure.facecolor': 'white',\n",
        " 'figure.figsize': (18, 10),\n",
        " 'lines.solid_capstyle': 'round',\n",
        " 'lines.linewidth': 2.5,\n",
        " 'patch.edgecolor': 'w',\n",
        " 'patch.force_edgecolor': True,\n",
        " 'text.color': 'black',\n",
        " 'legend.frameon': False,\n",
        " 'xtick.bottom': True,\n",
        " 'xtick.major.width': 3,\n",
        " 'xtick.major.size': 6,\n",
        " 'xtick.color': 'black',\n",
        " 'xtick.direction': 'out',\n",
        " 'xtick.top': False,\n",
        " 'ytick.color': 'black',\n",
        " 'ytick.direction': 'out',\n",
        " 'ytick.left': True,\n",
        " 'ytick.right': False,\n",
        " 'ytick.color' : 'black',\n",
        " 'ytick.major.width': 3,\n",
        " 'ytick.major.size': 6,\n",
        " 'axes.prop_cycle': plt.cycler(color=color_list),\n",
        " 'font.size': 16,\n",
        " 'font.family': 'serif'}\n",
        "plt.rcParams.update(cdds_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4Pr0OG2fC1t"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVIm3PljfJ3N"
      },
      "source": [
        "Load Data and Remove Seasonal Cycle\n",
        "\n",
        "##***Make sure to update your file location in google drive!!!***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCi8qitcfEDO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z4Q-VclfNf9"
      },
      "outputs": [],
      "source": [
        "#remove seasonal cycle\n",
        "def remove_time_mean(x):\n",
        "    return x - x.mean(dim='time')\n",
        "\n",
        "ds = ds.groupby('time.month').apply(remove_time_mean)\n",
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2Uc5rd9fUO9"
      },
      "source": [
        "##Trends\n",
        "\n",
        "Function to calculate and apply the trends for a time series over a specified time period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyi-8oO-fjJ1"
      },
      "outputs": [],
      "source": [
        "#historical period found in Fig 3\n",
        "historical_period = slice(\"1960\", \"2014\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvLEZd0qfVn1"
      },
      "outputs": [],
      "source": [
        "def get_trends(time_series, time_period):\n",
        "    \"\"\"Calculates and applies the trends for a time series over a specified time period\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    time_series: xarray `DataSet` object\n",
        "        `DataSet` with time series for variable, must have time as a data variable\n",
        "\n",
        "    time_period: string\n",
        "        the time period (in years) that you want the trends to be applied for\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    trends_ds: xarray `DataSet` object\n",
        "        applied trend for the time series over the time period, has \"tas_polyfit_coefficients\" as data variable with dimensions of model and degree\n",
        "    \"\"\"\n",
        "    #calculate trends for the time period\n",
        "    time = time_series.time\n",
        "\n",
        "    #sub-select for desired time period (historical in our case)\n",
        "    time_period = time_series.sel(time=time_period)\n",
        "\n",
        "    #convert to seconds and add to dataset\n",
        "    seconds = (time_period.time.astype(float).values / 1000000000)\n",
        "    time_series_seconds = time_period.copy()\n",
        "    time_series_seconds = time_series_seconds.assign(time = seconds)\n",
        "\n",
        "    #compute polyfit values\n",
        "    trends_ds = time_series_seconds.polyfit(dim = \"time\", deg = 1)\n",
        "\n",
        "\n",
        "    #return trends\n",
        "    return trends_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g93oT6Gyf9P9"
      },
      "source": [
        "##Sij\n",
        "\n",
        "described in methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqhKlsA1f7Tm"
      },
      "outputs": [],
      "source": [
        "#convert time to seconds store it as variable\n",
        "seconds = (ds.time.astype(float).values / 1000000000 )\n",
        "#calculate trends for the dataset over the historical period\n",
        "ds_trends = get_trends(ds, historical_period)\n",
        "\n",
        "#calculate the distance between models i and j for every model where j cycles through every model\n",
        "#making empty dataset with 27 models and 26 distances for each model\n",
        "Sij = np.zeros((ds.model.shape[0], ds.model.shape[0] - 1))\n",
        "distances = np.zeros((ds.model.shape[0] - 1))\n",
        "\n",
        "\n",
        "for i in range(ds.model.shape[0]) :\n",
        "#calcuate trend and mean for chosen i model\n",
        "  dT_i = ((seconds) * (ds_trends.tas_polyfit_coefficients[0, i].values))\n",
        "  dT_i = dT_i.mean()\n",
        "\n",
        "#remove i from chosen j models\n",
        "  for j in range(ds.model.shape[0] - 1) :\n",
        "    ds_copy = ds.copy()\n",
        "    removed_i= ds_copy.where(ds_copy.model != ds_copy.model[i], drop=True)\n",
        "\n",
        "#calculate trend and mean for chosen j model\n",
        "    removed_i_trends = get_trends(removed_i, historical_period)\n",
        "    dT_j = ((seconds) * (removed_i_trends.tas_polyfit_coefficients[0, j].values))\n",
        "    dT_j = dT_j.mean()\n",
        "\n",
        "#find distance between i and j model means\n",
        "    distances[j] = abs(dT_i - dT_j)\n",
        "\n",
        "#add the distance to Sij dataset\n",
        "  Sij[i] = distances\n",
        "\n",
        "\n",
        "#find median of Sij, which is important as a normalization value in the weighting process\n",
        "median = np.median(Sij)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhNGe67ChCyV"
      },
      "source": [
        "##Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6T7yaUYiObN"
      },
      "outputs": [],
      "source": [
        "def get_weights(trends, pseudo_ind, time_series, time_period, Sij, models, remove_pseudo=True):\n",
        "    \"\"\"Calculates the weights for a time series and given pseudo obsersation\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    trends: xarray `DataSet` object\n",
        "        applied trend for the time series over the time period, has \"tas_polyfit_coefficients\" as data variable with dimensions of model and degree\n",
        "\n",
        "    pseudo_ind: integer\n",
        "        the index value for the chosen pseudo observation model\n",
        "\n",
        "    time_series: xarray `DataSet` object\n",
        "        `DataSet` with time series for variable, must have time as a data variable\n",
        "\n",
        "    time_period: string\n",
        "        the time period (in years) that you want the trends to be applied for\n",
        "\n",
        "    Sij: array\n",
        "        calculated above, the distance between models i and j for every model\n",
        "\n",
        "    models: integer\n",
        "        the number of models after dropping pseudo, 26 for temperature\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    weights_normed: array\n",
        "        the weighted values of each model for the ensemble given in time_series\n",
        "    \"\"\"\n",
        "    #slice the time series to the specific time period and convert that time to seconds in order to apply the trends\n",
        "    time_period = time_series.sel(time=time_period)\n",
        "    seconds = (time_period.time.astype(float).values / 1000000000)\n",
        "\n",
        "    #find mean value of the chosen pseudo observation's trend\n",
        "    #important in finding the distances between the pseudo observation and chosen model\n",
        "    pseudo_trend = (seconds) * (trends.tas_polyfit_coefficients[0, pseudo_ind].values)\n",
        "    pseudo_mean = pseudo_trend.mean()\n",
        "\n",
        "    #below is normalization for sigma d and sigma s\n",
        "    #median_D is median of Sij at the pseudo_ind\n",
        "    median_D = np.median(Sij[pseudo_ind])\n",
        "\n",
        "    #median_S is the median of Sij without the pseudo observation included\n",
        "    Sij_pseudo_dropped = np.delete(Sij, pseudo_ind, axis=0)\n",
        "    median_S = np.median(Sij_pseudo_dropped)\n",
        "\n",
        "    #remove pseudo from ensemble so it is not weighted (defaultly set to true)\n",
        "    if remove_pseudo:\n",
        "      trends = trends.where(trends.model != trends.model[pseudo_ind], drop=True)\n",
        "\n",
        "    #calculate weights for each ensemble member based on equation\n",
        "    weights = list()\n",
        "    for source in range(len(trends.model)):\n",
        "        #find mean value of the chosen model's trend\n",
        "        dT_i = ((seconds) * (trends.tas_polyfit_coefficients[0, source].values))\n",
        "        dT_i_mean = dT_i.mean()\n",
        "\n",
        "        #find the distance between model i and observations, using the mean of the trends for each\n",
        "        distance = dT_i_mean - pseudo_mean\n",
        "\n",
        "        #sigma_d is a value previously optimized\n",
        "        sigma_d = 0.31\n",
        "        #calculate numerator of weighting equation, normalize with median_D\n",
        "        weight_numerator = np.exp(-(distance**2 / (median_D * sigma_d)**2))\n",
        "\n",
        "        #sigma_s is a value previously optimized\n",
        "        sigma_s = 0.01\n",
        "        #calculate denominator of weighting equation, normalize with median_S\n",
        "        weight_denominator = 1 + np.sum(np.exp(-(Sij[source]**2 /(median_S * sigma_s**2))))\n",
        "\n",
        "\n",
        "        #in practice, weight_denominator does not make a signficant difference in the weights\n",
        "        weight = weight_numerator / weight_denominator\n",
        "        weights.append(weight)\n",
        "\n",
        "    weights_reshape = np.reshape(weights, models,)\n",
        "\n",
        "    #normalize again by the sum of the weights\n",
        "    weights_normed = weights/np.sum(weights)\n",
        "\n",
        "\n",
        "\n",
        "    return weights_normed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnSoNkj0-z-s"
      },
      "source": [
        "##Obtain temperature weights and save to netCDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6huCiYq89Cd"
      },
      "outputs": [],
      "source": [
        "#create empty structure\n",
        "weights_save = np.zeros((ds.model.shape[0], ds.model.shape[0] - 1))\n",
        "\n",
        "#fill weights_save with weighted values for each pseudo observation choice\n",
        "for i in range(ds.model.shape[0]) :\n",
        "  weights_list = get_weights(ds_trends, i, ds, slice(\"1850\", \"2099\"), Sij, 26)\n",
        "  weights_save[i] = weights_list\n",
        "\n",
        "\n",
        "#export these weights to a netcdf to be used for regional figures\n",
        "weights = xr.DataArray(weights_save, name = \"weights\")\n",
        "weights = weights.to_dataset()\n",
        "weights = weights.rename_dims(dim_0 = 'pseudo_dropped')\n",
        "weights = weights.rename_dims(dim_1 = 'models')\n",
        "\n",
        "weights.to_netcdf(\"weights_temperature.nc\", mode = 'w', format = \"NETCDF4\", engine = 'netcdf4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbfOhBtafzgi"
      },
      "outputs": [],
      "source": [
        "#weights printed out\n",
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_ITQ-7L-vM1"
      },
      "source": [
        "##Format weights for precip and save to netCDF\n",
        "\n",
        "Precipitation has one less model (the model at index 5), so we recalculate weights without this model. These weights will be applied to global and regional precipitation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, Sij is recalcuated without the 5th index model. This make little to no difference between using the original Sij, but is good for consistency"
      ],
      "metadata": {
        "id": "SHDRlhEC0SmM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqsvfHFUmdtm"
      },
      "outputs": [],
      "source": [
        "#remove 5th index model\n",
        "ds_anom_formatted = ds.where(ds.model != ds.model[5], drop=True)\n",
        "\n",
        "#convert time to seconds store it as variable\n",
        "seconds = (ds_anom_formatted.time.astype(float).values / 1000000000 )\n",
        "#calculate trends for the dataset over the historical period\n",
        "ds_trends_dropped = get_trends(ds_anom_formatted, historical_period)\n",
        "\n",
        "#calculate the distance between models i and j for every model where j cycles through every model\n",
        "#making empty dataset with 26 models and 25 distances for each model\n",
        "Sij_dropped = np.zeros((ds_anom_formatted.model.shape[0], ds_anom_formatted.model.shape[0] - 1))\n",
        "distances_dropped = np.zeros((ds_anom_formatted.model.shape[0] - 1))\n",
        "\n",
        "\n",
        "for i in range(ds_anom_formatted.model.shape[0]) :\n",
        "#calcuate trend and mean for chosen i model\n",
        "  dT_i_dropped = ((seconds) * (ds_trends_dropped.tas_polyfit_coefficients[0, i].values))\n",
        "  dT_i_dropped = dT_i_dropped.mean()\n",
        "\n",
        "#remove i from chosen j models\n",
        "  for j in range(ds_anom_formatted.model.shape[0] - 1) :\n",
        "    ds_copy_dropped = ds_anom_formatted.copy()\n",
        "    removed_i_dropped = ds_copy_dropped.where(ds_copy_dropped.model != ds_copy_dropped.model[i], drop=True)\n",
        "\n",
        "#calculate trend and mean for chosen j model\n",
        "    removed_i_trends_dropped = get_trends(removed_i_dropped, historical_period)\n",
        "    dT_j_dropped = ((seconds) * (removed_i_trends_dropped.tas_polyfit_coefficients[0, j].values))\n",
        "    dT_j_dropped = dT_j_dropped.mean()\n",
        "\n",
        "#find distance between i and j model means\n",
        "    distances_dropped[j] = abs(dT_i_dropped - dT_j_dropped)\n",
        "\n",
        "#add the distance to Sij dataset\n",
        "  Sij_dropped[i] = distances_dropped\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1TJj_Os9T4N"
      },
      "outputs": [],
      "source": [
        "#make new weights structure for precip\n",
        "weights_precip_formatted = np.zeros((ds_trends_dropped.model.shape[0], ds_trends_dropped.model.shape[0] - 1))\n",
        "\n",
        "#calculate weights with model dropped data\n",
        "for i in range(ds_trends_dropped.model.shape[0]) :\n",
        "  weights_list_dropped_precip = get_weights(ds_trends_dropped, i, ds_anom_formatted, slice(\"1850\", \"2099\"), Sij_dropped, 25)\n",
        "  weights_precip_formatted[i] = weights_list_dropped_precip\n",
        "\n",
        "#export\n",
        "weights_formatted = xr.DataArray(weights_precip_formatted, name = \"weights\")\n",
        "weights_formatted = weights_formatted.to_dataset()\n",
        "weights_formatted = weights_formatted.rename_dims(dim_0 = 'pseudo_dropped')\n",
        "weights_formatted = weights_formatted.rename_dims(dim_1 = 'models')\n",
        "\n",
        "weights_formatted.to_netcdf(\"weights_temperature_reformatted.nc\", mode = 'w', format = \"NETCDF4\", engine = 'netcdf4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdz6qAEflZHm"
      },
      "outputs": [],
      "source": [
        "weights_formatted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWh18PLjB4Ds"
      },
      "source": [
        "##Apply weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCrR_M3Vpb5p"
      },
      "outputs": [],
      "source": [
        "W=list()\n",
        "UW=list()\n",
        "\n",
        "#apply weights for each pseudo choice\n",
        "for i_obs in range(27):\n",
        "    #select pseudo weights from the saved weights\n",
        "    weights=weights_save[i_obs,:]\n",
        "\n",
        "    #select pseudo from the data\n",
        "    ds_obs=ds.isel(model=i_obs)\n",
        "\n",
        "    #select all models but the pseudo from the data\n",
        "    v=np.delete(np.arange(27), i_obs)\n",
        "    ds_gcm=ds.isel(model=v)\n",
        "\n",
        "    #add the weights to the dataset\n",
        "    ds_gcm=ds_gcm.assign({\"weights\": (\"model\",weights)})\n",
        "\n",
        "    #apply the weights to the dataset\n",
        "    ds_gcm=ds_gcm.assign(tas_fut_weights=ds_gcm.tas*ds_gcm.weights)\n",
        "\n",
        "    #apply common baseline to the data\n",
        "    #weights become 0 after this line but thats okay bc we already applied the weights and dont use the weights again\n",
        "    ds_gcm = ds_gcm.sel(time=slice('2080', '2099')).mean(dim='time')-ds_gcm.sel(time=slice('1960', '2014')).mean(dim='time')\n",
        "\n",
        "    #apply common baseline to the pseudo\n",
        "    fut_period = ds_obs.sel(time=slice('2080', '2099')).mean(dim='time')-ds_obs.sel(time=slice('1960', '2014')).mean(dim='time')\n",
        "\n",
        "\n",
        "    #find weighted mean difference\n",
        "    y_w = np.abs(fut_period - ds_gcm.tas_fut_weights.sum('model'))\n",
        "\n",
        "    #find unweighted mean difference\n",
        "    y_uw =np.abs(fut_period - ds_gcm.tas.mean('model'))\n",
        "\n",
        "    #add to list for this pseudo choice and move onto the next pseudo choice\n",
        "    W.append(y_w)\n",
        "    UW.append(y_uw)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Y_W = xr.concat(W, dim = 'model',coords='minimal',compat='override')\n",
        "Y_UW = xr.concat(UW, dim = 'model',coords='minimal',compat='override')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI0HooiQpnfx"
      },
      "outputs": [],
      "source": [
        "#reformat the lists holding weighted and unweighted means for plotting\n",
        "weighted_distribution = Y_W.tas.values\n",
        "unweighted_distribution = Y_UW.tas.values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot RMSE"
      ],
      "metadata": {
        "id": "907nAUTONzEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdWG8lMm_cQk"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "#plotting weighted and unweighted histograms on the same figure\n",
        "sns.histplot(weighted_distribution, color=\"red\", label='weighted', kde=False, stat=\"density\", alpha=0.3, bins = 15)\n",
        "sns.histplot(unweighted_distribution, color=\"blue\", label='unweighted', kde=False, stat=\"density\", alpha=0.3, bins = 15)\n",
        "\n",
        "#density line\n",
        "sns.kdeplot(weighted_distribution, fill=False, color=\"red\")\n",
        "sns.kdeplot(unweighted_distribution, fill=False, color=\"blue\")\n",
        "\n",
        "plt.legend(loc='upper right', fontsize = 30)\n",
        "plt.xlim([0, 2])\n",
        "plt.ylim([0, 1.75])\n",
        "plt.xlabel(\"Error (K)\", fontsize = 30)\n",
        "plt.ylabel(\"Density\", fontsize = 30)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.tick_params(axis='both', labelsize=30)\n",
        "\n",
        "sns.despine(trim=True, offset=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2oFMtjYMG1-"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Temperature based on Temperature Weights Histogram\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "\n",
        "plt.hist(weighted_distribution, label=\"weighted distribution\", color = '#FF7F7F')\n",
        "plt.hist(unweighted_distribution, label=\"unweighted distribution\", color = '#1E90FF', alpha=0.7)\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare Variance"
      ],
      "metadata": {
        "id": "VWRmjXV96l8H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gd14mlnkncV"
      },
      "outputs": [],
      "source": [
        "(np.var(weighted_distribution)) / np.var(unweighted_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare Median"
      ],
      "metadata": {
        "id": "yjVm0UdI6n0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izUjr2fzyX4l"
      },
      "outputs": [],
      "source": [
        "(np.median(weighted_distribution)) / np.median(unweighted_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare RMSE"
      ],
      "metadata": {
        "id": "z7mNzUu96pdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28PpiIJIqf_e"
      },
      "outputs": [],
      "source": [
        "RMSE = ((weighted_distribution - weighted_distribution.mean())**2).mean()\n",
        "RMSE = np.sqrt(RMSE)\n",
        "RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipFCflTxj7sh"
      },
      "outputs": [],
      "source": [
        "RMSE_unweighted = ((unweighted_distribution - unweighted_distribution.mean())**2).mean()\n",
        "RMSE_unweighted = np.sqrt(RMSE_unweighted)\n",
        "RMSE_unweighted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSABcpt-j_av"
      },
      "outputs": [],
      "source": [
        "RMSE/RMSE_unweighted"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}